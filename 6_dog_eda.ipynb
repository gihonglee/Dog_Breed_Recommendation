{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Machine Learning / Time Series\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# Snowflake & ETC\n",
    "import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dog_data_clean.csv\", index_col= [0])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('temp', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup_set(input_list):\n",
    "    result = []\n",
    "    for ele in input_list:\n",
    "        if ele not in result:\n",
    "            result.append(ele)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.corr()\n",
    "\n",
    "strong_neg_list = []\n",
    "weak_neg_list = []\n",
    "strong_pos_list = []\n",
    "weak_pos_list = []\n",
    "\n",
    "for col in corr_df.columns:\n",
    "    strong_neg = np.where(corr_df[col] < -0.7)\n",
    "    weak_neg = np.where((corr_df[col] >= -0.7) & (corr_df[col] < -0.4))\n",
    "    strong_pos = np.where((corr_df[col] > 0.7) & (corr_df[col] != 1))\n",
    "    weak_pos = np.where((corr_df[col] <= 0.7) & (corr_df[col] > 0.4))\n",
    "\n",
    "    strong_neg_list_temp = list(corr_df.index[strong_neg])\n",
    "    weak_neg_list_temp = list(corr_df.index[weak_neg])\n",
    "    strong_pos_list_temp = list(corr_df.index[strong_pos])\n",
    "    weak_pos_list_temp = list(corr_df.index[weak_pos])\n",
    "\n",
    "    strong_neg_list_temp = [{ele,col} for ele in strong_neg_list_temp]\n",
    "    weak_neg_list_temp = [ {ele,col} for ele in weak_neg_list_temp]\n",
    "    strong_pos_list_temp = [{ele,col} for ele in strong_pos_list_temp]\n",
    "    weak_pos_list_temp = [{ele,col} for ele in weak_pos_list_temp]\n",
    "\n",
    "    strong_neg_list = strong_neg_list + strong_neg_list_temp\n",
    "    weak_neg_list = weak_neg_list + weak_neg_list_temp\n",
    "    strong_pos_list = strong_pos_list + strong_pos_list_temp\n",
    "    weak_pos_list = weak_pos_list + weak_pos_list_temp\n",
    "\n",
    "strong_neg_list  = remove_dup_set(strong_neg_list)\n",
    "weak_neg_list = remove_dup_set(weak_neg_list)\n",
    "strong_pos_list = remove_dup_set(strong_pos_list)\n",
    "weak_pos_list = remove_dup_set(weak_pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's focus on the one that are hightly correlated\n",
    "\n",
    "# what can we observe from here\n",
    "sns.set(font_scale=1.7)\n",
    "fig, ax = plt.subplots(figsize=(20,20))  \n",
    "sns.heatmap(df.corr(), ax=ax, annot = True, vmin = 0.4, vmax = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "high 0.7 ~\n",
    "moderate 0.4 ~ 0.7\n",
    "weak ~0.4\n",
    "\n",
    "There are high correlation between weight and height\n",
    "moderate correlation between\n",
    "- weight & Doorling\n",
    "- height & Drooling\n",
    "- open to stranger & playfulness\n",
    "- energy level & playfulness\n",
    "- mental stimulation needs & playfullness\n",
    "- mental stimulation needs & energy level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's focus on the one that are hightly correlated\n",
    "\n",
    "# what can we observe from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what can we observe from here\n",
    "sns.set(font_scale=1.7)\n",
    "fig, ax = plt.subplots(figsize=(20,20))  \n",
    "sns.heatmap(df.corr(), ax=ax, annot = True, vmin = -1, vmax = -0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factor = df.drop(['dog','color','marking','health','grooming','excercise','training','nutrition','Coat Type','Coat Length'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adequacy Test\n",
    "# Before you perform factor analysis, you need to evaluate the “factorability” of our dataset.\n",
    "# Factorability means \"can we found the factors in the dataset?\". There are two methods to check the factorability or sampling adequacy:\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(df_factor)\n",
    "chi_square_value, p_value\n",
    "\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(df_factor)\n",
    "kmo_model\n",
    "\n",
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(df_factor)\n",
    "ev, v = fa.get_eigenvalues()\n",
    "\n",
    "plt.scatter(np.arange(1,len(ev)+1),ev)\n",
    "plt.plot(np.arange(1,len(ev)+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('factors')\n",
    "plt.ylabel('eigenvalues of the factors')\n",
    "\n",
    "fa = FactorAnalyzer(3, rotation=\"varimax\")\n",
    "fa.fit(df_factor)\n",
    "fa.loadings_\n",
    "dir(fa)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(fa.loadings_, annot = True, yticklabels= df_factor.columns, cmap=\"rocket_r\", ax = ax)\n",
    "\n",
    "# df_summary = pd.DataFrame(fa.get_factor_variance())\n",
    "# df_summary.columns = ['Factor1','Factor2','Factor3']\n",
    "# df_summary.index = ['SS Loadings', 'Proportion Var','Cumulative Var']\n",
    "\n",
    "# df_summary\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# cols = df.columns.drop(['DATE','ECOMM_SALES_AMT', 'RETAIL_SALES_AMT'])\n",
    "# # Separating out the features\n",
    "# x = df[cols].values\n",
    "# # Standardizing the features\n",
    "# x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# principalComponents = pca.fit_transform(x)\n",
    "# principalDf = pd.DataFrame(data = principalComponents\n",
    "#              , columns = ['principal component 1', 'principal component 2'])\n",
    "# ecomm_series = df[['ECOMM_SALES_AMT']].reset_index(drop=True)\n",
    "# finalDf = pd.concat([principalDf, ecomm_series], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = df[['DATE'] + ecomm_variables]\n",
    "data = lag_df(data,ecomm_lag_dict)\n",
    "data = data.drop(['DATE'] + traffic_variables, axis = 1)\n",
    "feature_names = data.columns\n",
    "X = StandardScaler().fit_transform(data)\n",
    "\n",
    "# %%\n",
    "# Run factor analysis with Varimax rotation\n",
    "n_comps = 8\n",
    "\n",
    "methods = [\n",
    "    (\"PCA\", PCA()),\n",
    "    (\"Unrotated FA\", FactorAnalysis()),\n",
    "    (\"Varimax FA\", FactorAnalysis(rotation=\"varimax\")),\n",
    "]\n",
    "fig, axes = plt.subplots(ncols=len(methods), figsize=(10, 8))\n",
    "for ax, (method, fa) in  zip(axes,methods):\n",
    "    fa.set_params(n_components=n_comps)\n",
    "    fa.fit(X)\n",
    "\n",
    "    fa_loadings = fa.components_.T\n",
    "    # variance explained\n",
    "    total_var = X.var(axis=0).sum()  # total variance of original variables,\n",
    "                                    # equal to no. of vars if they are standardized\n",
    "    var_exp = np.sum(fa_loadings**2, axis=0)\n",
    "    prop_var_exp = var_exp/total_var\n",
    "    cum_prop_var_exp = np.cumsum(var_exp/total_var)\n",
    "    print(f\"variance explained: {var_exp.round(3)}\")\n",
    "    print(f\"proportion of variance explained: {prop_var_exp.round(3)}\")\n",
    "    print(f\"cumulative proportion of variance explained: {cum_prop_var_exp.round(3)}\")\n",
    "    ax.scatter(x = list(range(len(cum_prop_var_exp))) ,y = prop_var_exp)\n",
    "\n",
    "x= [1,2]\n",
    "y = [100,200]\n",
    "plt.scatter(x=x,y=y)\n",
    "\n",
    "# %%\n",
    "# Run factor analysis with Varimax rotation\n",
    "\n",
    "n_comps = 3\n",
    "\n",
    "methods = [\n",
    "    (\"PCA\", PCA()),\n",
    "    (\"Unrotated FA\", FactorAnalysis())\n",
    "    ,(\"Varimax FA\", FactorAnalysis(rotation=\"varimax\")),\n",
    "]\n",
    "fig, axes = plt.subplots(ncols=len(methods), figsize=(20, 8))\n",
    "i = 0\n",
    "for ax, (method, fa) in zip(axes, methods):\n",
    "    i += 1\n",
    "    fa.set_params(n_components=n_comps)\n",
    "    fa.fit(X)\n",
    "\n",
    "    components = fa.components_.T\n",
    "    print(\"\\n\\n %s :\\n\" % method)\n",
    "    print(components)\n",
    "    vmax = np.abs(components).max()\n",
    "    if i == 1:\n",
    "        sns.heatmap(components, ax= ax, vmax = vmax, vmin= -vmax, cmap=\"RdBu_r\", annot= True, yticklabels=feature_names)\n",
    "    else:\n",
    "        sns.heatmap(components, ax= ax, vmax = vmax, vmin= -vmax, cmap=\"RdBu_r\", annot= True, yticklabels=[])\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(str(method))\n",
    "fig.suptitle(\"Factors\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# %%\n",
    "\n",
    "# Run factor analysis with Varimax rotation\n",
    "\n",
    "n_comps = 3\n",
    "\n",
    "methods = [\n",
    "    (\"Unrotated FA\", FactorAnalysis())\n",
    "]\n",
    "fig, axes = plt.subplots(ncols=len(methods), figsize=(20, 8))\n",
    "i = 0\n",
    "for (method, fa) in methods:\n",
    "    i += 1\n",
    "    fa.set_params(n_components=n_comps)\n",
    "    fa.fit(X)\n",
    "\n",
    "    components = fa.components_.T\n",
    "    print(\"\\n\\n %s :\\n\" % method)\n",
    "    print(components)\n",
    "    vmax = np.abs(components).max()\n",
    "    sns.set(font_scale=2)\n",
    "    sns.heatmap(components, vmax = vmax, vmin= -vmax, cmap=\"RdBu_r\", annot= True, yticklabels=feature_names,xticklabels = [1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(str(method))\n",
    "fig.suptitle(\"Factors\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fa.score(X)\n",
    "m = fa.components_\n",
    "n = fa.noise_variance_\n",
    "m1 = m**2\n",
    "m2 = np.sum(m1,axis=1)\n",
    "pvar1 = (100*m2[0])/np.sum(m2)\n",
    "pvar2 = (100*m2[1])/np.sum(m2)\n",
    "pvar3 = (100*m2[2])/np.sum(m2)\n",
    "print(pvar1,pvar2,pvar3)\n",
    "print(pvar1 + pvar2 + pvar3)\n",
    "\n",
    "fa_loadings = fa.components_.T    # loadings\n",
    "\n",
    "# variance explained\n",
    "total_var = X.var(axis=0).sum()  # total variance of original variables,\n",
    "                                 # equal to no. of vars if they are standardized\n",
    "\n",
    "var_exp = np.sum(fa_loadings**2, axis=0)\n",
    "prop_var_exp = var_exp/total_var\n",
    "cum_prop_var_exp = np.cumsum(var_exp/total_var)\n",
    "\n",
    "print(f\"variance explained: {var_exp.round(3)}\")\n",
    "print(f\"proportion of variance explained: {prop_var_exp.round(3)}\")\n",
    "print(f\"cumulative proportion of variance explained: {cum_prop_var_exp.round(3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29f9ce33f6c884e3855952bfd7c8e1ec7ab3eef33d7abd85991a8524700c864f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
